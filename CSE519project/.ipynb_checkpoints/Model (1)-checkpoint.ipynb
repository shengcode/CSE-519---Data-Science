{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "import random\n",
    "import datetime\n",
    "from dateutil.parser import parse\n",
    "import numpy as np\n",
    "import itertools\n",
    "import string\n",
    "import time\n",
    "import csv\n",
    "import math\n",
    "from operator import itemgetter\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import permutation_test_score\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error, median_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.neighbors import KNeighborsRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Control Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV_File = 'billboard_w_song_facts_v2.csv'\n",
    "Year = 2018\n",
    "Quarter = 3\n",
    "test_size = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def advanced_insert_feature(df, func = None, feature_name = None):\n",
    "    if not feature_name:\n",
    "        raise ('Empty feature')\n",
    "\n",
    "    new_data = df.apply(lambda x: func(x, feature_name), axis = 1)\n",
    "    df.insert(df.shape[1], feature_name, new_data)\n",
    "    \n",
    "def insert_feature(df, func = None, feature_name = None):\n",
    "    if not feature_name:\n",
    "        raise ('Empty feature')\n",
    "\n",
    "    new_data = df.apply(func, axis = 1)\n",
    "    df.insert(df.shape[1], feature_name, new_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Features for base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function\n",
    "adjusted_diff_time = {0: 0, 1: 0.25, 2: 0.5, 3: 0.75}\n",
    "def adjust_time_duration(month):\n",
    "    quarter = (month - 1)//3\n",
    "    return adjusted_diff_time[quarter]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def difference_year(row_data, year = Year):\n",
    "    return year - row_data['year']\n",
    "\n",
    "def adjusted_difference_year(row_data, year = Year):\n",
    "    return year + adjust_time_duration(Quarter) - row_data['year'] - adjust_time_duration(row_data['weekid'].month)\n",
    "\n",
    "def log_rank(row_data):\n",
    "    return np.log(row_data['rank'])\n",
    "\n",
    "def log_reverse_rank(row_data):\n",
    "    return np.log(101 - row_data['rank'])\n",
    "\n",
    "def log_weeks_on_chart(row_data):\n",
    "    return np.log(row_data['weeks_on_chart'])\n",
    "\n",
    "def log_jump_time(row_data):\n",
    "    if row_data['jump_time'] == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return np.log(row_data['jump_time'])\n",
    "\n",
    "def log_max_duration(row_data):\n",
    "    if row_data['max_jump_duration'] == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return np.log(row_data['max_jump_duration'])\n",
    "    \n",
    "def log_popularity(row_data, rule):\n",
    "    if row_data[rule] == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return np.log(row_data[rule])\n",
    "\n",
    "def insert_base_feature(df):\n",
    "    insert_feature(df, difference_year, 'diff_year')\n",
    "    insert_feature(df, adjusted_difference_year, 'adjusted_diff_year')\n",
    "    insert_feature(df, log_rank, 'log_rank')\n",
    "    insert_feature(df, log_reverse_rank, 'log_reverse_rank')\n",
    "    insert_feature(df, log_weeks_on_chart, 'log_weeks_on_chart')\n",
    "    insert_feature(df, log_jump_time, 'log_jump_time')\n",
    "    insert_feature(df, log_max_duration, 'log_max_duration')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = pd.read_csv(CSV_File, sep='\\t')\n",
    "df_all['weekid'] = pd.to_datetime(df_all.weekid, infer_datetime_format = True)\n",
    "df_train = pd.read_csv('fixed_train.csv', sep = '\\t')\n",
    "df_test = pd.read_csv('fixed_test.csv', sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Features for advanced model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_feature(row_data, feature_name = None):\n",
    "    if not feature_name:\n",
    "        raise ('Empty feature')\n",
    "\n",
    "    songid = row_data['song']\n",
    "    \n",
    "    if df_features[feature].loc[df_features[feature]['song'] == songid].shape[0] > 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auxiliary Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_statistics_error_info(df, model_name, popularity, predicted_popularity):\n",
    "    print('Mean Absolute Error of' , model_name, 'is', mean_absolute_error(df[popularity], df[predicted_popularity]))\n",
    "    print('Median Absolute Error of' , model_name, 'is', median_absolute_error(df[popularity], df[predicted_popularity]))\n",
    "    print('Mean Squared Error of' , model_name, 'is', mean_squared_error(df[popularity], df[predicted_popularity]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_error_by_rank(train = df_train, test = None, err_type = 'mean_absolute_error', compared_feature = ['strict_rule', 'predict'], model_name = ''):\n",
    "    err = {'mean_squared_error': 'Mean Squared Error',\n",
    "           'mean_absolute_error': 'Mean Absolute Error',\n",
    "           'median_absolute_error': 'Median Absolute Error'}\n",
    "    func = {'mean_squared_error': mean_squared_error,\n",
    "           'mean_absolute_error': mean_absolute_error,\n",
    "           'median_absolute_error': median_absolute_error}\n",
    "\n",
    "    if err_type not in err.keys():\n",
    "        print('Please enter valid error type')\n",
    "        return\n",
    "\n",
    "    predicted_group_by_rank=train.groupby('rank')\n",
    "    train_rank_error=[]\n",
    "    for index, group in predicted_group_by_rank:\n",
    "        train_rank_error.append(func[err_type](group[compared_feature[0]], group[compared_feature[1]]))\n",
    "    \n",
    "    plt.figure(figsize=(20, 6), dpi=80)\n",
    "    x = np.arange(1, 101)\n",
    "\n",
    "    if test is None:\n",
    "        plt.bar(x + 0.1, train_rank_error, color = 'green', width = 0.8, label='train')\n",
    "    else:\n",
    "        predicted_group_by_rank=test.groupby('rank')\n",
    "        test_rank_error = []\n",
    "        for index, group in predicted_group_by_rank:\n",
    "            test_rank_error.append(func[err_type](group[compared_feature[0]], group[compared_feature[1]]))\n",
    "            \n",
    "        plt.bar(x + 0.1, train_rank_error, color = 'green', width = 0.4, label='train')\n",
    "        plt.bar(x + 0.5, test_rank_error, color = 'orange', width = 0.4, label='test')\n",
    "    plt.xlabel(\"Peak Rank\",fontsize=18)\n",
    "    plt.ylabel(\"Mean Absolute Error\",fontsize=18)\n",
    "    plt.title(\"Mean Absolute Error by Rank (\" + model_name + \")\",fontsize=18)\n",
    "    plt.rc('xtick',labelsize=14)\n",
    "    plt.rc('ytick',labelsize=14)\n",
    "    ax = plt.gca()\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.legend()\n",
    "    plt.show()\n",
    "\n",
    "def plot_mae_by_year(train = df_train, test = None, err_type = 'mean_absolute_error', compared_feature = ['strict_rule', 'predict'], model_name = ''):\n",
    "    predicted_group_by_rank=train.groupby('year')\n",
    "    min_year=df_train['year'].min()\n",
    "    max_year=df_train['year'].max()+1  \n",
    "    year = np.arange(min_year, max_year)\n",
    "    train_rank_error=[]\n",
    "    for index, group in predicted_group_by_rank:\n",
    "        train_rank_error.append(mean_absolute_error(group['strict_rule'], group['predict'])) \n",
    "    \n",
    "    plt.figure(figsize=(20, 6), dpi=80)\n",
    "\n",
    "    if test is None:\n",
    "        plt.bar(year + 0.1, train_rank_error, color = 'green', width = 0.8, label='train')\n",
    "    else:\n",
    "        predicted_group_by_rank=test.groupby('year')\n",
    "        test_rank_error = []\n",
    "        for index, group in predicted_group_by_rank:\n",
    "            test_rank_error.append(mean_absolute_error(group['strict_rule'], group['predict']))\n",
    "\n",
    "        plt.bar(year + 0.1, train_rank_error, color = 'green', width = 0.4, label='train')\n",
    "        plt.bar(year + 0.5, test_rank_error, color = 'orange', width = 0.4, label='test')\n",
    "    plt.xlabel(\"Year\",fontsize=18)\n",
    "    plt.ylabel(\"Mean Absolute Error\",fontsize=18)\n",
    "    plt.title(\"Mean Absolute Error by Year (Base Model)\",fontsize=18)\n",
    "    plt.rc('xtick',labelsize=14)\n",
    "    plt.rc('ytick',labelsize=14)\n",
    "    ax = plt.gca()\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base Model Used in Progress Report\n",
    "\n",
    "$popularity = rank \\times \\Delta t^{C_t}$\n",
    "\n",
    "$\\log(popularity) = \\log(rank) +  C_{t}\\log(\\Delta t)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['log_diff_year'] = df_train['diff_year'].apply(lambda x: np.log(x))\n",
    "df_test['log_diff_year'] = df_test['diff_year'].apply(lambda x: np.log(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_one(train = df_train, test = df_test, x_features = None, y_feature = None, rank_type = 'log_reverse_rank'):\n",
    "    x_train = train[x_features].values\n",
    "    y_train = train[y_feature].values\n",
    "    log_y_train = train.apply(lambda row: -row[rank_type] if row[y_feature] == 0 else np.log(row[y_feature]) - row[rank_type], axis = 1).values\n",
    "    x_test = test[x_features].values\n",
    "    y_test = test[y_feature].values\n",
    "    log_y_test = test.apply(lambda row: -row[rank_type] if row[y_feature] == 0 else np.log(row[y_feature]) - row[rank_type], axis = 1).values\n",
    "    \n",
    "    regr = LinearRegression()  \n",
    "\n",
    "    regr.fit(x_train.reshape(-1, 1), log_y_train)\n",
    "\n",
    "    log_y_test_pred = regr.predict(x_test.reshape(-1, 1))\n",
    "    log_y_train_pred= regr.predict(x_train.reshape(-1, 1))\n",
    "\n",
    "    y_test_pred = pd.Series(log_y_test_pred).combine(pd.Series(test[rank_type].values), lambda x1, x2: 100 if x1+x2 > 100 else x1+x2)\n",
    "    y_train_pred = pd.Series(log_y_train_pred).combine(pd.Series(train[rank_type].values), lambda x1, x2: 100 if x1+x2 > 100 else x1+x2)\n",
    "\n",
    "    info = {'coef': regr.coef_, \\\n",
    "            'test': {'Log-MSE': mean_squared_error(log_y_test, log_y_test_pred), \\\n",
    "                     'Log-MAE': mean_absolute_error(log_y_test, log_y_test_pred), \\\n",
    "                     'Log-Variance': r2_score(log_y_test, log_y_test_pred), \\\n",
    "                     'MSE': mean_squared_error(y_test, y_test_pred), \\\n",
    "                     'MAE': mean_absolute_error(y_test, y_test_pred), \\\n",
    "                     'Variance': r2_score(y_test, y_test_pred)},\\\n",
    "            'train': {'Log-MSE': mean_squared_error(log_y_train, log_y_train_pred), \\\n",
    "                     'Log-MAE': mean_absolute_error(log_y_train, log_y_train_pred), \\\n",
    "                     'Log-Variance': r2_score(log_y_train, log_y_train_pred), \\\n",
    "                     'MSE': mean_squared_error(y_train, y_train_pred), \\\n",
    "                     'MAE': mean_absolute_error(y_train, y_train_pred), \\\n",
    "                     'Variance': r2_score(y_train, y_train_pred)}}\n",
    "\n",
    "    return {'info':info, \\\n",
    "            'train':train.reset_index().assign(predicted=y_train_pred), \\\n",
    "            'test':test.reset_index().assign(predicted=y_test_pred)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "stats = {'strict_rule': {}, 'popularity': {}}\n",
    "mae_error = {'train': [], 'test': []}\n",
    "mse_error = {'train': [], 'test': []}\n",
    "for _ in range(1):\n",
    "    MAE = {'strict_rule': [], 'popularity': []}\n",
    "    MSE = {'strict_rule': [], 'popularity': []}\n",
    "    for y_feature in ['strict_rule']:\n",
    "        for x_features in ['log_diff_year']:\n",
    "            for rank in range(1, 101):\n",
    "                model_info = model_one(train = df_train.loc[(df_train[y_feature] > 0)& (df_train['rank'] == rank)] , \\\n",
    "                                test = df_test.loc[(df_test[y_feature] > 0) & (df_test['rank'] == rank)], \\\n",
    "                                x_features = x_features, \\\n",
    "                                y_feature = y_feature, \\\n",
    "                                rank_type = 'log_reverse_rank')\n",
    "                info = model_info['info']\n",
    "                MAE[y_feature].append([x_features, \\\n",
    "                                   info['coef'], \\\n",
    "                                   info['train']['MAE'], \\\n",
    "                                   info['train']['Variance'], \\\n",
    "                                   info['test']['MAE'], \\\n",
    "                                   info['test']['Variance']])\n",
    "                mae_error['train'].append(info['train']['MAE'])\n",
    "                mse_error['train'].append(info['train']['MSE'])\n",
    "                mae_error['test'].append(info['test']['MAE'])\n",
    "                mse_error['test'].append(info['test']['MSE'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df_train1, df_test1 = train_test_split(df_all, test_size = test_size)\n",
    "df_train1['log_diff_year'] = df_train1['diff_year'].apply(lambda x: np.log(x))\n",
    "df_test1['log_diff_year'] = df_test1['diff_year'].apply(lambda x: np.log(x))\n",
    "model_one_info = model_one(train = df_train1.loc[(df_train1['strict_rule'] > 0)],\n",
    "                           test = df_test1.loc[(df_train1['strict_rule'] > 0)],\n",
    "                           x_features = 'log_diff_year',\n",
    "                           y_feature = 'strict_rule',\n",
    "                           rank_type = 'log_reverse_rank')\n",
    "print_linear_regr_info(model_one_info['info'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plt.figure(figsize=(20, 6), dpi=80)\n",
    "x = np.arange(0, 100)\n",
    "plt.bar(x,mae_error['train'], color = 'orange', width = 0.25, label='train')\n",
    "plt.bar(x + 0.4, mae_error['test'], color = 'blue', width = 0.25, label='test')\n",
    "plt.xlabel(\"Peak Rank\",fontsize=18)\n",
    "plt.ylabel(\"Absolute Error of Popularity\",fontsize=18)\n",
    "plt.title(\"Absolute Error of Every Year\",fontsize=18)\n",
    "plt.rc('xtick',labelsize=14)\n",
    "plt.rc('ytick',labelsize=14)\n",
    "ax = plt.gca()\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plt.figure(figsize=(20, 6), dpi=80)\n",
    "x = np.arange(100)\n",
    "plt.bar(x,mse_error['train'], color = 'y', width = 0.25, label='train')\n",
    "plt.bar(x + 0.4, mse_error['test'], color = 'r', width = 0.25, label='test')\n",
    "plt.xlabel(\"Peak Rank\",fontsize=18)\n",
    "plt.ylabel(\"Absolute Error of Popularity\",fontsize=18)\n",
    "plt.title(\"Absolute Error of Every Year\",fontsize=18)\n",
    "plt.rc('xtick',labelsize=14)\n",
    "plt.rc('ytick',labelsize=14)\n",
    "ax = plt.gca()\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression Base Model\n",
    "$\n",
    "\\text{popularity} = C_i\\text{(reverse_rank)}^{C_r}(\\text{weeks_on_chart})^{C_w}e^{C_t (\\text{diff_year})}\n",
    "$\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\log(\\text{popularity}) =& \\log C_i + C_r\\log(\\text{reverse_rank}) + C_w\\log(\\text{weeks_on_chart}) + {C_t (\\text{diff_year})}\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_linear_regression(train = df_train, test = df_test, x_features = None, y_feature = None, intercept = True):\n",
    "    x_train = train[x_features].values\n",
    "    y_train = train[y_feature].values\n",
    "    log_y_train = train[y_feature].apply(lambda x: 0 if x == 0 else np.log(x)).values\n",
    "    x_test = test[x_features].values\n",
    "    y_test = test[y_feature].values\n",
    "    log_y_test = test[y_feature].apply(lambda x: 0 if x == 0 else np.log(x)).values\n",
    "    \n",
    "    regr = LinearRegression(fit_intercept=intercept)  \n",
    "    \n",
    "    regr.fit(x_train, log_y_train)\n",
    "    \n",
    "    log_y_test_pred = regr.predict(x_test)\n",
    "    log_y_train_pred= regr.predict(x_train)\n",
    "\n",
    "    y_test_pred = pd.Series(log_y_test_pred).apply(lambda x: np.exp(x))\n",
    "    y_train_pred = pd.Series(log_y_train_pred).apply(lambda x: np.exp(x))\n",
    "    \n",
    "    info = {'coef': regr.coef_, \\\n",
    "            'intercept': regr.intercept_, \\\n",
    "            'test': {'Log-MSE': mean_squared_error(log_y_test, log_y_test_pred), \\\n",
    "                     'Log-MAE': mean_absolute_error(log_y_test, log_y_test_pred), \\\n",
    "                     'Log-Variance': r2_score(log_y_test, log_y_test_pred), \\\n",
    "                     'MSE': mean_squared_error(y_test, y_test_pred), \\\n",
    "                     'MAE': mean_absolute_error(y_test, y_test_pred), \\\n",
    "                     'Variance': r2_score(y_test, y_test_pred)},\\\n",
    "            'train': {'Log-MSE': mean_squared_error(log_y_train, log_y_train_pred), \\\n",
    "                     'Log-MAE': mean_absolute_error(log_y_train, log_y_train_pred), \\\n",
    "                     'Log-Variance': r2_score(log_y_train, log_y_train_pred), \\\n",
    "                     'MSE': mean_squared_error(y_train, y_train_pred), \\\n",
    "                     'MAE': mean_absolute_error(y_train, y_train_pred), \\\n",
    "                     'Variance': r2_score(y_train, y_train_pred)}} \n",
    "\n",
    "    return {'info':info, \\\n",
    "            'regr':regr, \\\n",
    "            'train':train.reset_index().assign(predicted=y_train_pred), \\\n",
    "            'test':test.reset_index().assign(predicted=y_test_pred)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train1, df_test1 = train_test_split(df_all, test_size = 0.2)\n",
    "# df_train_wo_jump = df_train.loc[df_train.jump_time == 0]\n",
    "# df_test_wo_jump = df_test.loc[df_test.jump_time == 0]\n",
    "# df_train_w_jump = df_train.loc[df_train.jump_time > 0]\n",
    "# df_test_w_jump = df_test.loc[df_test.jump_time > 0]\n",
    "\n",
    "# print(df_train_wo_jump.shape[0], df_test_wo_jump.shape[0], df_train_w_jump.shape[0], df_test_w_jump.shape[0])\n",
    "# wo_jump_model_info = model_linear_regression(train=df_train_wo_jump.loc[df_train_wo_jump['popularity'] > 0], \n",
    "#                                              test=df_test_wo_jump.loc[df_test_wo_jump['popularity'] > 0], \n",
    "#                                              x_features=['log_reverse_rank', 'log_weeks_on_chart', 'diff_year'], \n",
    "#                                              y_feature='popularity',\n",
    "#                                              intercept=True)\n",
    "# print_linear_regr_info(wo_jump_model_info['info'])\n",
    "# wo_jump_model_info = model_linear_regression(train=df_train_wo_jump.loc[df_train_wo_jump['popularity'] > 0], \n",
    "#                                              test=df_test_wo_jump.loc[df_test_wo_jump['popularity'] > 0], \n",
    "#                                              x_features=['log_reverse_rank', 'log_weeks_on_chart', 'log_jump_time', 'log_max_duration','diff_year'], \n",
    "#                                              y_feature='popularity',\n",
    "#                                              intercept=True)\n",
    "# print_linear_regr_info(wo_jump_model_info['info'])\n",
    "# print('\\n\\n')\n",
    "# w_jump_model_info = model_linear_regression(train=df_train_w_jump.loc[df_train_w_jump['popularity'] > 0],\n",
    "#                                             test=df_test_w_jump.loc[df_test_w_jump['popularity'] > 0],\n",
    "#                                             x_features=['log_reverse_rank', 'log_weeks_on_chart', 'diff_year'],\n",
    "#                                             y_feature='popularity',\n",
    "#                                             intercept=True)\n",
    "# print_linear_regr_info(w_jump_model_info['info'])\n",
    "# w_jump_model_info = model_linear_regression(train=df_train_w_jump.loc[df_train_w_jump['popularity'] > 0],\n",
    "#                                             test=df_test_w_jump.loc[df_test_w_jump['popularity'] > 0],\n",
    "#                                             x_features=['log_reverse_rank', 'log_weeks_on_chart', 'log_jump_time', 'log_max_duration','diff_year'], \n",
    "#                                             y_feature='popularity',\n",
    "#                                             intercept=True)\n",
    "# print_linear_regr_info(w_jump_model_info['info'])\n",
    "# model_info = model_linear_regression(train=df_train.loc[df_train['popularity'] > 0],\n",
    "#                                      test=df_test.loc[df_test['popularity'] > 0],\n",
    "#                                      x_features=['log_reverse_rank', 'log_weeks_on_chart', 'diff_year'],\n",
    "#                                      y_feature='popularity',\n",
    "#                                      intercept=True)\n",
    "# print_linear_regr_info(model_info['info'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Possible Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "possible_x_features = [['log_reverse_rank', 'log_weeks_on_chart', 'diff_year']]\n",
    "possible_y_features = ['strict_rule']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $\\text{Base Model - Linear Regression}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "Mean Absolute Error of linear regression base model is 12.5059248215\n",
      "Median Absolute Error of linear regression base model is 10.0\n",
      "Mean Squared Error of linear regression base model is 255.211253237\n",
      "\n",
      " test\n",
      "Mean Absolute Error of linear regression base model is 12.6325320012\n",
      "Median Absolute Error of linear regression base model is 10.0\n",
      "Mean Squared Error of linear regression base model is 261.324383391\n"
     ]
    }
   ],
   "source": [
    "def predict_value_by_linear_regression(row_data):\n",
    "    rank = row_data['rank']\n",
    "    x_val = row_data[model_1_x_features].values\n",
    "    val = np.exp(model_1_regr.predict([x_val]))\n",
    "    \n",
    "    if val > 100:\n",
    "        val = 100\n",
    "    elif val < 0:\n",
    "        val = 0\n",
    "    else:\n",
    "        val = val.round(decimals = 0)\n",
    "    \n",
    "    return val\n",
    "\n",
    "model_1_x_features = possible_x_features[0]\n",
    "model_1_y_feature = possible_y_features[0]\n",
    "base_model_info = model_linear_regression(train = df_train.loc[(df_train[model_1_y_feature] > 0)] ,\n",
    "                                          test = df_test.loc[(df_test[model_1_y_feature] > 0)],\n",
    "                                          x_features = model_1_x_features,\n",
    "                                          y_feature = model_1_y_feature,\n",
    "                                          intercept = True)\n",
    "model_1_regr = base_model_info['regr']\n",
    "df_train['model_1_predicted'] = df_train.apply(predict_value_by_linear_regression, axis=1)\n",
    "df_test['model_1_predicted'] = df_test.apply(predict_value_by_linear_regression, axis=1)\n",
    "\n",
    "# Calcluate the Mean Absolute Error, Median Absolute Error, Mean Squared Error\n",
    "print('train')\n",
    "print_statistics_error_info(df_train.loc[df_train[model_1_y_feature] > 0], 'linear regression base model', model_1_y_feature, 'model_1_predicted')\n",
    "print('\\n test')\n",
    "print_statistics_error_info(df_test.loc[df_test[model_1_y_feature] > 0], 'linear regression base model', model_1_y_feature, 'model_1_predicted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $\\text{Base Model - Linear Regression}_r$\n",
    "- Build the linear regression model for every rank\n",
    "- Predict the song popularity\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "Mean Absolute Error of linear regression_r base model is 10.9584870125\n",
      "Median Absolute Error of linear regression_r base model is 9.0\n",
      "Mean Squared Error of linear regression_r base model is 199.025190301\n",
      "\n",
      " test\n",
      "Mean Absolute Error of linear regression_r base model is 11.274742429\n",
      "Median Absolute Error of linear regression_r base model is 9.0\n",
      "Mean Squared Error of linear regression_r base model is 210.023415548\n"
     ]
    }
   ],
   "source": [
    "def predict_value_by_rank(row_data):\n",
    "    rank = row_data['rank']\n",
    "    x_val = row_data[model_1_x_features].values\n",
    "    val = np.exp(regr_coef_per_rank[rank].predict([x_val]))\n",
    "    \n",
    "    if val > 100:\n",
    "        val = 100\n",
    "    elif val < 0:\n",
    "        val = 0\n",
    "    else:\n",
    "        val = val.round(decimals = 0)\n",
    "    \n",
    "    return val\n",
    "\n",
    "regr_coef_per_rank = {}\n",
    "train_stats = {'MAE':[], 'MSE':[]}\n",
    "test_stats = {'MAE':[], 'MSE':[]}\n",
    "\n",
    "for rank in range(1, 101):\n",
    "    model_info = model_linear_regression(train = df_train.loc[(df_train[model_1_y_feature] > 0) & (df_train['rank'] == rank)] , \\\n",
    "                           test = df_test.loc[(df_test[model_1_y_feature] > 0) & (df_test['rank'] == rank)], \\\n",
    "                           x_features = model_1_x_features, \\\n",
    "                           y_feature = model_1_y_feature, \\\n",
    "                           intercept = True)\n",
    "    regr_coef_per_rank[rank] = model_info['regr']\n",
    "\n",
    "df_train['model_1_r_predicted'] = df_train.apply(predict_value_by_rank, axis=1)\n",
    "df_test['model_1_r_predicted'] = df_test.apply(predict_value_by_rank, axis=1)\n",
    "\n",
    "# Calcluate the Mean Absolute Error, Median Absolute Error, Mean Squared Error\n",
    "print('train')\n",
    "print_statistics_error_info(df_train.loc[df_train[model_1_y_feature] > 0], 'linear regression_r base model', model_1_y_feature, 'model_1_r_predicted')\n",
    "print('\\n test')\n",
    "print_statistics_error_info(df_test.loc[df_test[model_1_y_feature] > 0], 'linear regression_r base model', model_1_y_feature, 'model_1_r_predicted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $\\text{ML - Random Forest}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_forest_machine_learning(df_train, df_test, x_features = None, y_feature = None, max_depth=2, random_state=0, n_estimators=100):\n",
    "    X=np.array(df_train[x_features].values)\n",
    "    y=np.array(df_train[y_feature].values)\n",
    "    regr = RandomForestRegressor(max_depth=max_depth, random_state=0,n_estimators=100)\n",
    "    regr.fit(X, y)\n",
    "    if DebugMode:\n",
    "        print(regr.feature_importances_)\n",
    "    X_test=np.array(df_test[feature_to_keep].values)\n",
    "    pred_test=regr.predict(X_test)\n",
    "    df_test.insert(df_test.shape[1], column='pred_randomForest', value=np.exp(pred_test) ) \n",
    "    \n",
    "    pred_train=regr.predict(X)\n",
    "    df_train.insert(df_train.shape[1], column='pred_randomForest', value=np.exp(pred_train) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## random forest with max_depth 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## random forest with max_depth 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plot_error_by_rank(df_train.loc[df_train[y_feature] > 0], \n",
    "                 df_test.loc[df_test[y_feature] > 0],\n",
    "                 err_type = 'median_absolute_error',\n",
    "                 compared_feature = [y_feature, 'predict'],\n",
    "                 model_name = '')\n",
    "\n",
    "plot_error_by_rank(df_train.loc[df_train[y_feature] > 0], \n",
    "                 df_test.loc[df_test[y_feature] > 0],\n",
    "                 err_type = 'mean_absolute_error',\n",
    "                 compared_feature = [y_feature, 'predict'],\n",
    "                 model_name = '')\n",
    "\n",
    "plot_error_by_rank(df_train.loc[df_train[y_feature] > 0], \n",
    "                 df_test.loc[df_test[y_feature] > 0],\n",
    "                 err_type = 'mean_squared_error',\n",
    "                 compared_feature = [y_feature, 'predict'],\n",
    "                 model_name = '')\n",
    "# plot_mae_by_year(df_test.loc[df_test['strict_rule'] > 0], df_train.loc[df_train['strict_rule'] > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 6), dpi=80)\n",
    "x = np.arange(1, 101)\n",
    "plt.bar(x,train_stats['MAE'], color = 'green', width = 0.4, label='train')\n",
    "plt.bar(x + 0.4, test_stats['MAE'], color = 'orange', width = 0.4, label='test')\n",
    "plt.xlabel(\"Peak Rank\",fontsize=18)\n",
    "plt.ylabel(\"Popularity Difference\",fontsize=18)\n",
    "plt.title(\"Mean Absolute Error by Rank (Base Model)\",fontsize=18)\n",
    "plt.rc('xtick',labelsize=14)\n",
    "plt.rc('ytick',labelsize=14)\n",
    "ax = plt.gca()\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(mean_absolute_error(df_train['strict_rule'], df_train['predict']))\n",
    "print(mean_squared_error(df_train['strict_rule'], df_train['predict']))\n",
    "print(mean_absolute_error(df_test['strict_rule'], df_test['predict']))\n",
    "print(mean_squared_error(df_test['strict_rule'], df_test['predict']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 6), dpi=80)\n",
    "x = np.arange(1, 101)\n",
    "plt.bar(x,train_stats['MAE'], color = 'green', width = 0.4, label='train')\n",
    "plt.bar(x + 0.4, test_stats['MAE'], color = 'orange', width = 0.4, label='test')\n",
    "plt.xlabel(\"Peak Rank\",fontsize=18)\n",
    "plt.ylabel(\"Popularity Difference\",fontsize=18)\n",
    "plt.title(\"Mean Absolute Error by Rank (Base Model)\",fontsize=18)\n",
    "plt.rc('xtick',labelsize=14)\n",
    "plt.rc('ytick',labelsize=14)\n",
    "ax = plt.gca()\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(20, 6), dpi=80)\n",
    "x = np.arange(100)\n",
    "plt.bar(x,train_stats['MSE'], color = 'y', width = 0.4, label='train')\n",
    "plt.bar(x + 0.4, test_stats['MSE'], color = 'r', width = 0.4, label='test')\n",
    "plt.xlabel(\"Peak Rank\",fontsize=18)\n",
    "plt.ylabel(\"Popularity Difference\",fontsize=18)\n",
    "plt.title(\"Mean Square Error of Popularity by Rank (Base Model)\",fontsize=18)\n",
    "plt.rc('xtick',labelsize=14)\n",
    "plt.rc('ytick',labelsize=14)\n",
    "ax = plt.gca()\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_features = [['log_rank', 'log_weeks_on_chart', 'log_jump_time', 'log_max_duration', 'diff_year'],\n",
    "              ['log_reverse_rank', 'log_weeks_on_chart', 'log_jump_time', 'log_max_duration', 'diff_year'],\n",
    "              ['log_rank', 'log_weeks_on_chart', 'log_jump_time', 'log_max_duration', 'adjusted_diff_year'],\n",
    "              ['log_reverse_rank', 'log_weeks_on_chart', 'log_jump_time', 'log_max_duration', 'adjusted_diff_year']]\n",
    "y_features = ['strict_rule']\n",
    "\n",
    "# df_train, df_test = train_test_split(df_all, test_size = test_size)\n",
    "model_info_stats = {}\n",
    "stats = {}\n",
    "for idx, x_feature in enumerate(x_features):\n",
    "    for y_feature in y_features:\n",
    "#         for rank in range(1, 100):\n",
    "        model_info = model_two(train = df_train.loc[(df_train[y_feature] > 0) & (df_train['rank'] == rank)] , \\\n",
    "                               test = df_test.loc[(df_test[y_feature] > 0) & (df_test['rank'] == rank)], \\\n",
    "                               x_features = x_feature, \\\n",
    "                               y_feature = y_feature, \\\n",
    "                               intercept = True)\n",
    "        if idx not in stats:\n",
    "            stats[idx] = {'train':[], 'test':[]}\n",
    "        stats[idx]['train'].append(model_info['info']['train']['MAE'])\n",
    "        stats[idx]['test'].append(model_info['info']['test']['MAE'])  \n",
    "\n",
    "        if idx not in model_info_stats:\n",
    "            model_info_stats[idx] = []\n",
    "        model_info_stats[idx].append(model_info)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = {'strict_rule': {}, 'popularity': {}}\n",
    "mae_error = {'train': [], 'test': []}\n",
    "mse_error = {'train': [], 'test': []}\n",
    "\n",
    "    \n",
    "MAE = {'strict_rule': [], 'popularity': []}\n",
    "MSE = {'strict_rule': [], 'popularity': []}\n",
    "for y_feature in ['strict_rule']:\n",
    "    for x_features in [['log_reverse_rank', 'log_weeks_on_chart', 'log_jump_time', 'log_max_duration', 'diff_year']]:\n",
    "        for rank in range(1, 101):\n",
    "            model_info = model_two(train = df_train_wo_jump.loc[(df_train_wo_jump[y_feature] > 0)& (df_train_wo_jump['rank'] == rank)] , \\\n",
    "                            test = df_test_wo_jump.loc[(df_test_wo_jump[y_feature] > 0) & (df_test_wo_jump['rank'] == rank)], \\\n",
    "                            x_features = x_features, \\\n",
    "                            y_feature = y_feature, \\\n",
    "                            intercept = True)\n",
    "\n",
    "            mae_error['train'].append(model_info['info']['train']['MAE'])\n",
    "            mse_error['train'].append(model_info['info']['train']['MSE'])\n",
    "            mae_error['test'].append(model_info['info']['test']['MAE'])\n",
    "            mse_error['test'].append(model_info['info']['test']['MSE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Model I\n",
    "$\n",
    "\\text{popularity} = \\text{rank}^{C_r}(\\text{weeks_on_chart})^{C_w}(\\text{jump_time})^{C_j} (\\text{max_duration})^{C_m}e^{C_t \\Delta t + C_i}\n",
    "$\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\log(\\text{popularity}) =& C_r\\log(\\text{rank}) + C_w\\log(\\text{weeks_on_chart}) \\\\&+ C_j\\log(\\text{jump_time}) + C_m\\log(\\text{max_duration}) + {C_t \\Delta t} + C_i\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for y_feature in ['strict_rule']:\n",
    "    for x_features in [['log_reverse_rank', 'log_weeks_on_chart', 'log_jump_time', 'log_max_duration', 'diff_year']]:\n",
    "#         for rank in range(1, 101):\n",
    "            model_info = model_two(train = df_train.loc[(df_train[y_feature] > 0)& (df_train['rank'] == rank)] , \\\n",
    "                            test = df_test.loc[(df_test[y_feature] > 0) & (df_test['rank'] == rank)], \\\n",
    "                            x_features = x_features, \\\n",
    "                            y_feature = y_feature, \\\n",
    "                            intercept = True)\n",
    "\n",
    "            mae_error['train'].append(model_info['info']['train']['MAE'])\n",
    "            mse_error['train'].append(model_info['info']['train']['MSE'])\n",
    "            mae_error['test'].append(model_info['info']['test']['MAE'])\n",
    "            mse_error['test'].append(model_info['info']['test']['MSE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 6), dpi=80)\n",
    "x = np.arange(0, 100)\n",
    "plt.bar(x,mae_error['train'], color = 'green', width = 0.25, label='train')\n",
    "plt.bar(x + 0.4, mae_error['test'], color = 'orange', width = 0.25, label='test')\n",
    "plt.xlabel(\"Peak Rank\",fontsize=18)\n",
    "plt.ylabel(\"Popularity Difference\",fontsize=18)\n",
    "plt.title(\"Mean Absolute Error by Rank (Base Model)\",fontsize=18)\n",
    "plt.rc('xtick',labelsize=14)\n",
    "plt.rc('ytick',labelsize=14)\n",
    "ax = plt.gca()\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 6), dpi=80)\n",
    "x = np.arange(100)\n",
    "plt.bar(x,mse_error['train'], color = 'y', width = 0.25, label='train')\n",
    "plt.bar(x + 0.4, mse_error['test'], color = 'r', width = 0.25, label='test')\n",
    "plt.xlabel(\"Peak Rank\",fontsize=18)\n",
    "plt.ylabel(\"Popularity Difference\",fontsize=18)\n",
    "plt.title(\"Mean Square Error of Popularity by Rank (Base Model)\",fontsize=18)\n",
    "plt.rc('xtick',labelsize=14)\n",
    "plt.rc('ytick',labelsize=14)\n",
    "ax = plt.gca()\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_info['train']['pop_difference'] = model_info['train'].apply(lambda row: row.strict_rule - row.predicted, axis = 1)\n",
    "model_info['train']['abs_pop_difference'] = model_info['train']['pop_difference'].apply(lambda val: np.abs(val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_info['train'].sort_values(by=['pop_difference'],inplace=True)\n",
    "underperform=model_info['train'].head(10)\n",
    "overperform=model_info['train'].tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(mae_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_info_stats[2][0]['train']['year']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank = 13\n",
    "plt.scatter(model_info_stats[1]['train']['year'], model_info_stats[2]['train']['strict_rule'],  color='blue')\n",
    "plt.scatter(model_info_stats[1]['train']['year'], model_info_stats[2]['train']['predicted'],  color='red')\n",
    "# plt.plot(model_info_stats[2][0]['train']['year'], model_info_stats[2][0]['train']['popularity'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Feature Combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['log_weeks_on_chart', 'log_jump_time', 'log_max_duration']\n",
    "features_set_w_dy = []\n",
    "features_set_w_ady = []\n",
    "\n",
    "for idx in range(len(features) + 1):\n",
    "    set_combinations = itertools.combinations(features, idx)\n",
    "    for subset in set_combinations:\n",
    "        features_set_w_dy.append(list(subset))\n",
    "        features_set_w_ady.append(list(subset))\n",
    "        features_set_w_dy[-1].extend(['log_rank', 'diff_year'])\n",
    "        features_set_w_ady[-1].extend(['log_rank', 'adjusted_diff_year'])\n",
    "\n",
    "Experiment_Mode = False\n",
    "if not Experiment_Mode:\n",
    "    print(len(features_set_w_dy))\n",
    "    for features_opt0, features_opt1 in zip (features_set_w_dy, features_set_w_ady):\n",
    "        print(features_opt0)\n",
    "        print(features_opt1, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "possible_x_features = features_set_w_ady + features_set_w_dy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "possible_x_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log_rank, log_weeks_on_chart, log_jump_time, log_max_duration, diff_year, adjusted_diff_year\n",
    "stats = {'strict_rule':{}, 'lenient_rule':{}, 'popularity':{}}\n",
    "mae_error = {'train':[], 'test':[]}\n",
    "for _ in range(20):\n",
    "    df_train, df_test = train_test_split(df_all, test_size = test_size)\n",
    "    \n",
    "    MAE = {'strict_rule':[], 'lenient_rule':[], 'popularity':[]}\n",
    "    y_features = ['strict_rule', 'lenient_rule', 'popularity']\n",
    "    y_features = ['strict_rule']\n",
    "    for y_feature in y_features:\n",
    "#         print('y features:', y_feature)\n",
    "#         print()\n",
    "#         test_x_features = [['log_weeks_on_chart','log_jump_time','log_max_duration','log_rank','adjusted_diff_year']]\n",
    "        for x_features in possible_x_features:\n",
    "    #         print('    x features:', x_features)\n",
    "            model_info = model_two(train = df_train.loc[(df_train[y_feature] > 0)] , \\\n",
    "                                           test = df_test.loc[(df_test[y_feature] > 0)], \\\n",
    "                                           x_features = x_features, \\\n",
    "                                           y_feature = y_feature, \\\n",
    "                                           intercept = True)\n",
    "#             for rank in range(1, 101):\n",
    "#                 model_info = model_two(train = df_train.loc[(df_train[y_feature] > 0) & (df_train['rank'] == rank)] , \\\n",
    "#                                            test = df_test.loc[(df_test[y_feature] > 0) & (df_test['rank'] == rank)], \\\n",
    "#                                            x_features = x_features, \\\n",
    "#                                            y_feature = y_feature, \\\n",
    "#                                            intercept = True)\n",
    "            info = model_info['info']\n",
    "            mae_error['train'].append(info['train']['MAE'])\n",
    "            mae_error['test'].append(info['test']['MAE'])\n",
    "            MAE[y_feature].append([x_features, \\\n",
    "                                   info['coef'], \\\n",
    "                                   info['intercept'], \\\n",
    "                                   info['train']['MAE'], \\\n",
    "                                   info['train']['MSE'], \\\n",
    "                                   info['train']['Variance'], \\\n",
    "                                   info['test']['MAE'], \\\n",
    "                                   info['test']['MSE'], \\\n",
    "                                   info['test']['Variance']])\n",
    "            if x_features == ['log_rank', 'diff_year'] or \\\n",
    "               x_features == ['log_rank', 'adjusted_diff_year']:\n",
    "                print('base model:', \\\n",
    "                      x_features, '\\n'\\\n",
    "                      'coef:', info['coef'], info['intercept'], '\\n', \\\n",
    "                      info['train']['MAE'], \\\n",
    "                      info['train']['MSE'], \\\n",
    "                      info['train']['Variance'], \\\n",
    "                      info['test']['MAE'], \\\n",
    "                      info['test']['MSE'], \\\n",
    "                      info['test']['Variance'], '\\n')\n",
    "\n",
    "        local_info = min(MAE[y_feature],key=itemgetter(3))\n",
    "        x_features_str = '-'.join(local_info[0])\n",
    "        if x_features_str in stats[y_feature]:\n",
    "            stats[y_feature][x_features_str] += 1\n",
    "        else:\n",
    "            stats[y_feature][x_features_str] = 1\n",
    "#         print('minimal error model:', \\\n",
    "#               local_info[0], '\\n', \\\n",
    "#               'coef:', local_info[1:3], '\\n', \\\n",
    "#               local_info[3:], '\\n')\n",
    "\n",
    "\n",
    "    #         print('\\n')\n",
    "    #     print('\\n')\n",
    "    # print(df_train['popularity'].values.shape)\n",
    "    # print(df_train['strict_rule'].apply(lambda x: 0 if x == 0 else np.log(x)).values.shape)\n",
    "    # list(df_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(mae_error['train'])\n",
    "plt.plot(mae_error['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for major_key in stats.keys():\n",
    "    print(major_key)\n",
    "    for minor_key in stats[major_key].keys():\n",
    "        print(minor_key, stats[major_key][minor_key])\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_absolute_error(predict_train, predict_test):\n",
    "     # calculate absolute error of each year \n",
    "    predicted_test_year = predict_test.groupby('year')\n",
    "    min_year = predict_test['year'].min()\n",
    "    max_year = predict_test['year'].max()+1    \n",
    "    \n",
    "    error_year=[]\n",
    "    for index, group in predicted_test_year:\n",
    "        error= np.array(group['popularity']-group['predictedPopularity'])\n",
    "        length=error.shape[0]\n",
    "        error=np.abs(error)\n",
    "        total_error=error.sum() \n",
    "        total_error=total_error/length\n",
    "        error_year.append(total_error) \n",
    "    \n",
    "    year = range(min_year, max_year)\n",
    "    \n",
    "    print(\"the mean absolute error of different years is: \")\n",
    "    print(sum(error_year)/len(error_year))\n",
    "    plt.figure(figsize=(20, 6), dpi=80)\n",
    "    plt.plot(year,error_year)\n",
    "    plt.xlabel(\"Year\")\n",
    "    plt.ylabel(\"Absolute Error\")\n",
    "    plt.title(\"for test case\")\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    # PLOT BAR FIRUGE     \n",
    "    plt.figure(figsize=(20, 6), dpi=80)\n",
    "    plt.bar(year,error_year)\n",
    "    plt.xlabel(\"Year\",fontsize=18)\n",
    "    plt.ylabel(\"Absolute Error\",fontsize=18)\n",
    "    plt.title(\"Absolute Error of Every Year\",fontsize=18)\n",
    "    plt.rc('xtick',labelsize=14)\n",
    "    plt.rc('ytick',labelsize=14)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add New Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def advanced_insert_feature(df, func = None, feature_name = None):\n",
    "    if not feature_name:\n",
    "        raise ('Empty feature')\n",
    "\n",
    "    new_data = df.apply(lambda x: func(x, feature_name), axis = 1)\n",
    "    df.insert(df.shape[1], feature_name, new_data)\n",
    "    \n",
    "def insert_feature(df, func = None, feature_name = None):\n",
    "    if not feature_name:\n",
    "        raise ('Empty feature')\n",
    "\n",
    "    new_data = df.apply(func, axis = 1)\n",
    "    df.insert(df.shape[1], feature_name, new_data)\n",
    "\n",
    "def new_feature(row_data, feature_name = None):\n",
    "    if not feature_name:\n",
    "        raise ('Empty feature')\n",
    "\n",
    "    songid = row_data['song']\n",
    "    \n",
    "    if df_features[feature].loc[df_features[feature]['song'] == songid].shape[0] > 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_features = {}\n",
    "csv_files = [f for f in os.listdir('.') if os.path.isfile(f)]\n",
    "for csv_file in csv_files:\n",
    "    if 'csv' in csv_file and 'songs' in csv_file:\n",
    "        feature = csv_file.split('.')[0]\n",
    "        df_features[feature] = pd.read_csv(csv_file)\n",
    "        df_features[feature]['songid'] = df_features[feature][['song', 'artist']].apply(lambda x: ''.join(x), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df_features = {}\n",
    "csv_files = ['songs-used-in-movies.csv',\\\n",
    "            'songs-used-in-tv-shows.csv',\\\n",
    "            'songs-used-in-commercials.csv']\n",
    "for csv_file in csv_files:\n",
    "    feature = csv_file.split('.')[0]\n",
    "    df_features[feature] = pd.read_csv(csv_file)\n",
    "    df_features[feature]['songid'] = df_features[feature][['song', 'artist']].apply(lambda x: ''.join(x), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in df_features.keys():\n",
    "    advanced_insert_feature(df_train, new_feature, feature)\n",
    "    advanced_insert_feature(df_test, new_feature, feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new fit feature\n",
    "for feature in df_features.keys():\n",
    "    new_feature = 'fit-' + feature\n",
    "    df_train[new_feature] = df_train.apply(lambda row: np.log(row[feature] + 1), axis=1)\n",
    "    df_test[new_feature] = df_test.apply(lambda row: np.log(row[feature] + 1), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['new_feature'] = df_train[list(df_features)].apply(lambda x: 1 if any(x) else 0, axis=1)\n",
    "df_test['new_feature'] = df_test[list(df_features)].apply(lambda x: 1 if any(x) else 0, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['sum_feature'] = df_train[list(df_features)].apply(lambda x: np.log(sum(x)) if any(x) else 0, axis=1)\n",
    "df_test['sum_feature'] = df_test[list(df_features)].apply(lambda x: np.log(sum(x)) if any(x) else 0, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['log_weeks_on_charts'] = df_train['weeks_on_chart'].apply(lambda x: np.log(x))\n",
    "df_test['log_weeks_on_charts'] = df_test['weeks_on_chart'].apply(lambda x: np.log(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['log_jump_time'] = df_train['jump_time'].apply(lambda x: np.log(x**2) if x > 0 else 0)\n",
    "df_test['log_jump_time'] = df_test['jump_time'].apply(lambda x: np.log(x**2) if x > 0 else 0)\n",
    "df_train['log_max_jump_duration'] = df_train['max_jump_duration'].apply(lambda x: np.log(x**2) if x > 0 else 0)\n",
    "df_test['log_max_jump_duration'] = df_test['max_jump_duration'].apply(lambda x: np.log(x**2) if x > 0 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(df_features.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model -fit-movie, fit-tv-show, fit-commercials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation_featured_model(train = df_train, test = df_test, x_features = None, y_feature = None, intercept = True, full_info = False):\n",
    "    x_train = train[x_features].values  \n",
    "    y_train = train[y_feature].values\n",
    "    x_test = test[x_features].values\n",
    "    y_test = test[y_feature].values\n",
    "    \n",
    "    regr = linear_model.LinearRegression(fit_intercept=intercept) # with intercept \n",
    "    regr.fit(x_train, y_train)\n",
    "    ytest_pred = regr.predict(x_test)\n",
    "    ytrain_pred=regr.predict(x_train)\n",
    "    \n",
    "    if full_info:\n",
    "        print('Coefficients: \\n', regr.coef_)\n",
    "        print('Intercept when fit_intercept=True : {:.5f}'.format(regr.intercept_))\n",
    "    \n",
    "    print(\"Mean squared error for test case is: %.3f\"% mean_squared_error(y_test, ytest_pred))\n",
    "    if full_info:\n",
    "        print('Variance score for test case is: %.3f' % r2_score(y_test, ytest_pred))\n",
    "    print(\"Mean squared error for train case is: %.3f\"% mean_squared_error(y_train, ytrain_pred))\n",
    "    if full_info:\n",
    "        print('Variance score for test case is: %.3f' % r2_score(y_train, ytrain_pred))\n",
    "\n",
    "# df_train_mid=df_train.assign(predictedlogP=pd.Series(ytrain_pred))\n",
    "# df_test_mid=df_test.assign(predictedlogP=pd.Series(ytest_pred))\n",
    "    \n",
    "# predict_train=df_train_mid.assign(predictedPopularity=pd.Series( np.exp(df_train_mid['predictedlogP'])))\n",
    "# predict_test=df_test_mid.assign(predictedPopularity=pd.Series( np.exp(df_test_mid['predictedlogP'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_features = ['fit-'+ feature for feature in df_features.keys()]\n",
    "print(fit_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_year = 1958\n",
    "end_year = 2005\n",
    "for rank in ['logRank']:\n",
    "    for pop in ['logPopularity','logNewPopularity']:\n",
    "        print(rank, pop)\n",
    "        \n",
    "\n",
    "        print('linear regression - base model')\n",
    "        evaluation_featured_model(train=df_train.loc[df_train.year.isin([start_year, end_year])], test=df_test, x_features=[rank, 'Y_year'], y_feature=pop)\n",
    "\n",
    "        print('\\nlinear regression - base model 1')\n",
    "        evaluation_featured_model(train=df_train[df_train.year.isin([start_year, end_year])], test=df_test,x_features=[rank, 'Y_year', 'log_weeks_on_charts'], y_feature=pop)\n",
    "        \n",
    "        print('\\nlinear regression - base model 2')\n",
    "        evaluation_featured_model(train=df_train[df_train.year.isin([start_year, end_year])],\\\n",
    "                                  test=df_test\\\n",
    "                                  ,x_features=[rank, 'Y_year', 'log_weeks_on_charts'],\\\n",
    "                                  y_feature=pop)\n",
    "        \n",
    "        print('\\nlinear regression - base model 3')\n",
    "        evaluation_featured_model(train=df_train[df_train.year.isin([start_year, end_year])],\\\n",
    "                                  test=df_test,\\\n",
    "                                  x_features=[rank,\\\n",
    "                                              'Y_year',\\\n",
    "                                              'log_weeks_on_charts',\\\n",
    "                                              'log_jump_time'],\\\n",
    "                                  y_feature=pop)\n",
    "        \n",
    "        print('\\nlinear regression - base model 4')\n",
    "        evaluation_featured_model(train=df_train[df_train.year.isin([start_year, end_year])],\\\n",
    "                                  test=df_test,\\\n",
    "                                  x_features=[rank,\\\n",
    "                                              'Y_year',\\\n",
    "                                              'log_weeks_on_charts',\\\n",
    "                                              'log_jump_time',\\\n",
    "                                              'log_max_jump_duration'],\\\n",
    "                                  y_feature=pop)\n",
    "        \n",
    "        print('\\nlinear regression - base model 4')\n",
    "        evaluation_featured_model(train=df_train[df_train.year.isin([start_year, end_year])],\\\n",
    "                                  test=df_test,\\\n",
    "                                  x_features=[rank,\\\n",
    "                                              'Y_year',\\\n",
    "                                              'log_weeks_on_charts',\\\n",
    "                                              'log_jump_time',\\\n",
    "                                              'log_max_jump_duration',\\\n",
    "                                              'all_time_greatest_artist'],\\\n",
    "                                  y_feature=pop)\n",
    "        \n",
    "        print('\\nlinear regression - advanced model')\n",
    "        features = [rank, 'Y_year', 'log_weeks_on_charts', 'log_jump_time', 'log_max_jump_duration']\n",
    "        features.extend(fit_features)\n",
    "        evaluation_featured_model(train=df_train[df_train.year.isin([start_year, end_year])], test=df_test,x_features=features, y_feature=pop)\n",
    "        \n",
    "        print('\\nlinear regression - all as one model')\n",
    "        evaluation_featured_model(train=df_train[df_train.year.isin([start_year, end_year])], test=df_test,x_features=[rank, 'Y_year', 'log_weeks_on_charts', 'log_jump_time', 'log_max_jump_duration','new_feature'], y_feature=pop)\n",
    "        \n",
    "        print('\\nlinear regression - sum all feature model')\n",
    "        evaluation_featured_model(train=df_train[df_train.year.isin([start_year, end_year])], test=df_test,x_features=[rank, 'Y_year', 'log_weeks_on_charts', 'log_jump_time', 'log_max_jump_duration','sum_feature'], y_feature=pop)\n",
    "        print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_train.loc[(df_train['songs-used-in-commercials'] == 1)].shape[0])\n",
    "print(df_train.loc[(df_train['songs-used-in-movies'] == 1)].shape[0])\n",
    "print(df_train.loc[(df_train['songs-used-in-tv-shows'] == 1)].shape[0])\n",
    "print(df_train.loc[(df_train['songs-used-in-movies'] == 1) |\\\n",
    "                   (df_train['songs-used-in-tv-shows'] == 1) |\\\n",
    "                   (df_train['songs-used-in-commercials'] == 1)].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_features.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = df_train[list(df_features.keys())].corr()\n",
    "corr.style.background_gradient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [rank, 'Y_year', 'log_weeks_on_charts', 'log_jump_time', 'log_max_jump_duration']\n",
    "features.extend(fit_features)\n",
    "features.remove('fit-songs-for-wedding-anniversaries')\n",
    "corr = df_train[features].corr()\n",
    "corr.style.background_gradient()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNeighbors Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neigh = KNeighborsRegressor(n_neighbors=3)\n",
    "df_train, df_test = train_test_split(df_all, test_size = test_size)\n",
    "\n",
    "x_features = [['log_rank', 'log_weeks_on_chart', 'log_jump_time', 'log_max_duration', 'diff_year'],\n",
    "              ['log_reverse_rank', 'log_weeks_on_chart', 'log_jump_time', 'log_max_duration', 'diff_year'],\n",
    "              ['log_rank', 'log_weeks_on_chart', 'log_jump_time', 'log_max_duration', 'adjusted_diff_year'],\n",
    "              ['log_reverse_rank', 'log_weeks_on_chart', 'log_jump_time', 'log_max_duration', 'adjusted_diff_year']]\n",
    "y_features = ['strict_rule']\n",
    "\n",
    "\n",
    "neigh.fit(df_train[['rank', 'weeks_on_chart', 'jump_time', 'max_jump_duration', 'year']], df_train[['popularity']])\n",
    "\n",
    "train_predict_res = neigh.predict(df_train[['rank', 'weeks_on_chart', 'jump_time', 'max_jump_duration', 'year']])\n",
    "test_predict_res = neigh.predict(df_test[['rank', 'weeks_on_chart', 'jump_time', 'max_jump_duration', 'year']])\n",
    "\n",
    "print(mean_squared_error(df_train[['popularity']], train_predict_res),\n",
    "      mean_absolute_error(df_train[['popularity']], train_predict_res))\n",
    "print(mean_squared_error(df_test[['popularity']], test_predict_res),\n",
    "      mean_absolute_error(df_test[['popularity']], test_predict_res))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=5,\n",
       "           max_features='auto', max_leaf_nodes=None,\n",
       "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "           min_samples_leaf=1, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "           oob_score=False, random_state=0, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regr = RandomForestRegressor(max_depth=5, random_state=0, n_estimators=10)\n",
    "x_features = ['rank', 'weeks_on_chart', 'jump_time', 'max_jump_duration', 'year', 'all_time_greatest_artist']\n",
    "y_feature = 'strict_rule'\n",
    "regr.fit(df_train[x_features], df_train[y_feature])\n",
    "\n",
    "# print(regr.feature_importances_)\n",
    "\n",
    "# train_predict_res = regr.predict(df_train[x_features])\n",
    "# test_predict_res = regr.predict(df_test[x_features])\n",
    "\n",
    "# print(mean_squared_error(df_train[y_feature], train_predict_res),\n",
    "#       mean_absolute_error(df_train[y_feature], train_predict_res))\n",
    "# print(mean_squared_error(df_test[y_feature], test_predict_res),\n",
    "#       mean_absolute_error(df_test[y_feature], test_predict_res))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = SVR(gamma='scale', C=1.0, epsilon=0.2)\n",
    "x_features = ['rank', 'weeks_on_chart', 'jump_time', 'max_jump_duration', 'year', 'all_time_greatest_artist']\n",
    "y_feature = 'strict_rule'\n",
    "clf.fit(df_train[x_features], df_train[y_feature]) \n",
    "train_predict_res = clf.predict(df_train[x_features])\n",
    "test_predict_res = clf.predict(df_test[x_features])\n",
    "\n",
    "print(mean_squared_error(df_train[y_feature], train_predict_res),\n",
    "      mean_absolute_error(df_train[y_feature], train_predict_res))\n",
    "print(mean_squared_error(df_test[y_feature], test_predict_res),\n",
    "      mean_absolute_error(df_test[y_feature], test_predict_res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
